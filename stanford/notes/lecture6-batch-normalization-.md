# Batch Normalization

Another approach is just turning the outputs of each layer into a gaussian using some basic gaussian making math.

More specifically, you'd turn each dimension of your previous input into a gaussian and then output it. No need to cross-reference between dimensions. Ypu do this though sing the batch-wide mean, and batch-wide standard deviation, not just the mean and SD for the current image.

If you want to do this you need to add extra layers, which would be called batch-normalization layers.

You usually insert these after fully connected or conv layers, before the activation layers.

If you do this right it prevents the depth of a network from normalizing further and further at each layer (until everything is nearly 0).

More in Loffe and Szegedy 2015

## Learned parameters

You can also effect the way these gaussians are generated by adding some learned parameters to their math. It's hard to tell exactly how gaussifying everything will effect the network. It could even make things worse, so adding learned parameters will organically reduce the amount of gaussification where needed.

## In practice

This is hella useful.


## In testing

At test time you don't compute a mean/deviation for the test data, instead you gather an overall mean of all activations (for that layer for that dimension) during training and use this mean instead.